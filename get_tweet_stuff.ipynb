{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweeterpy\n",
      "  Downloading tweeterpy-1.0.7-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: six==1.16.0 in d:\\anaconda\\envs\\py3.10\\lib\\site-packages (from tweeterpy) (1.16.0)\n",
      "Requirement already satisfied: idna==3.4 in d:\\anaconda\\envs\\py3.10\\lib\\site-packages (from tweeterpy) (3.4)\n",
      "Requirement already satisfied: lxml==4.9.2 in d:\\anaconda\\envs\\py3.10\\lib\\site-packages (from tweeterpy) (4.9.2)\n",
      "Requirement already satisfied: charset-normalizer==3.1.0 in d:\\anaconda\\envs\\py3.10\\lib\\site-packages (from tweeterpy) (3.1.0)\n",
      "Collecting certifi==2023.7.22\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "     -------------------------------------- 158.3/158.3 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting urllib3==2.0.3\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "     -------------------------------------- 123.6/123.6 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting requests==2.31.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.6/62.6 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting bs4==0.0.1\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting demjson3==3.0.6\n",
      "  Downloading demjson3-3.0.6.tar.gz (131 kB)\n",
      "     ---------------------------------------- 131.5/131.5 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Brotli==1.0.9\n",
      "  Downloading Brotli-1.0.9-cp310-cp310-win_amd64.whl (383 kB)\n",
      "     ------------------------------------- 383.3/383.3 kB 12.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4==4.12.2 in d:\\anaconda\\envs\\py3.10\\lib\\site-packages (from tweeterpy) (4.12.2)\n",
      "Collecting soupsieve==2.4.1\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: bs4, demjson3\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1264 sha256=d2732182128ce4eb307d6aa6b37469c51bc8d1fa2eed9b139e92a1836f035100\n",
      "  Stored in directory: c:\\users\\rafit\\appdata\\local\\pip\\cache\\wheels\\e4\\62\\1d\\d4d1bc4f33350ff84227f89b258edb552d604138e3739f5c83\n",
      "  Building wheel for demjson3 (setup.py): started\n",
      "  Building wheel for demjson3 (setup.py): finished with status 'done'\n",
      "  Created wheel for demjson3: filename=demjson3-3.0.6-py3-none-any.whl size=75333 sha256=9855a07f6bb315fee13006b81bfa5dde4b6fac6206ee3e3f44adeff6640b31aa\n",
      "  Stored in directory: c:\\users\\rafit\\appdata\\local\\pip\\cache\\wheels\\bf\\cc\\85\\b97e01da795a059d6e82ca0f77b3da366a51f6c645f12c5a04\n",
      "Successfully built bs4 demjson3\n",
      "Installing collected packages: demjson3, Brotli, urllib3, soupsieve, certifi, requests, bs4, tweeterpy\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.4\n",
      "    Uninstalling soupsieve-2.4:\n",
      "      Successfully uninstalled soupsieve-2.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "Successfully installed Brotli-1.0.9 bs4-0.0.1 certifi-2023.7.22 demjson3-3.0.6 requests-2.31.0 soupsieve-2.4.1 tweeterpy-1.0.7 urllib3-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tweeterpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter password  =AJ#~Z+cBUR&7r/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweeterpy import TweeterPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 19:33:29,310 [\u001b[0;32mINFO\u001b[0m] :: API Updated Successfully.\n"
     ]
    }
   ],
   "source": [
    "twitter = TweeterPy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter.get_liked_tweets('rngland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter.get_friends('rngland', following=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 19:35:03,129 [\u001b[0;32mINFO\u001b[0m] :: User is authenticated.\n",
      "2023-09-22 19:35:03,130 [\u001b[0;32mINFO\u001b[0m] :: User is authenticated.\n",
      "2023-09-22 19:35:03,702 [\u001b[1;31mERROR\u001b[0m] :: This request requires a matching csrf cookie and header.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Github\\TweeterPy\\tweeterpy\\request_util.py\", line 32, in make_request\n",
      "    return util.check_for_errors(response.json())\n",
      "  File \"d:\\Github\\TweeterPy\\tweeterpy\\util.py\", line 85, in check_for_errors\n",
      "    raise Exception(error_message)\n",
      "Exception: This request requires a matching csrf cookie and header.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "This request requires a matching csrf cookie and header.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mme.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m a:\n\u001b[1;32m----> 2\u001b[0m     json\u001b[39m.\u001b[39mdump(twitter\u001b[39m.\u001b[39;49mget_user_info(\u001b[39m'\u001b[39;49m\u001b[39mrngland\u001b[39;49m\u001b[39m'\u001b[39;49m), a)\n",
      "File \u001b[1;32md:\\Github\\TweeterPy\\tweeterpy\\tweeterpy.py:130\u001b[0m, in \u001b[0;36mTweeterPy.login_decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     logger\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mUser is not authenticated.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogin()\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m original_function(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Github\\TweeterPy\\tweeterpy\\tweeterpy.py:244\u001b[0m, in \u001b[0;36mTweeterPy.get_user_info\u001b[1;34m(self, user_id)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m@login_decorator\u001b[39m\n\u001b[0;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_user_info\u001b[39m(\u001b[39mself\u001b[39m, user_id):\n\u001b[0;32m    236\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Extracts user details like username, userid, bio, website, follower/following count etc.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[39m        dict: User information.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m     user_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_user_id(user_id)\n\u001b[0;32m    245\u001b[0m     variables \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m\"\u001b[39m: user_id, \u001b[39m\"\u001b[39m\u001b[39mwithSafetyModeUserFields\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[0;32m    246\u001b[0m     request_payload \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_request_data(\n\u001b[0;32m    247\u001b[0m         Path\u001b[39m.\u001b[39mUSER_INFO_ENDPOINT, variables, user_data_features\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Github\\TweeterPy\\tweeterpy\\tweeterpy.py:231\u001b[0m, in \u001b[0;36mTweeterPy.get_user_id\u001b[1;34m(self, username)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_user_data(username)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrest_id\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    229\u001b[0m request_payload \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_request_data(\n\u001b[0;32m    230\u001b[0m     Path\u001b[39m.\u001b[39mUSER_ID_ENDPOINT, {\u001b[39m\"\u001b[39m\u001b[39mscreen_name\u001b[39m\u001b[39m\"\u001b[39m: username})\n\u001b[1;32m--> 231\u001b[0m response \u001b[39m=\u001b[39m make_request(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_payload)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39muser_result_by_screen_name\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mrest_id\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Github\\TweeterPy\\tweeterpy\\request_util.py:46\u001b[0m, in \u001b[0;36mmake_request\u001b[1;34m(url, session, method, max_retries, timeout, skip_error_checking, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m api_limit_stats\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrate_limit_exhausted\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     45\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRate Limit Exceeded => \u001b[39m\u001b[39m{\u001b[39;00mapi_limit_stats\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "File \u001b[1;32md:\\Github\\TweeterPy\\tweeterpy\\request_util.py:32\u001b[0m, in \u001b[0;36mmake_request\u001b[1;34m(url, session, method, max_retries, timeout, skip_error_checking, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m skip_error_checking:\n\u001b[0;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mcheck_for_errors(response\u001b[39m.\u001b[39;49mjson())\n\u001b[0;32m     33\u001b[0m response_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m     34\u001b[0m     [line\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39mstrip()])\n\u001b[0;32m     35\u001b[0m response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32md:\\Github\\TweeterPy\\tweeterpy\\util.py:85\u001b[0m, in \u001b[0;36mcheck_for_errors\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     83\u001b[0m         error_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([error[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     84\u001b[0m                                    \u001b[39mfor\u001b[39;00m error \u001b[39min\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[1;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(error_message)\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mException\u001b[0m: This request requires a matching csrf cookie and header."
     ]
    }
   ],
   "source": [
    "# with open(\"me.json\", \"w\") as a:\n",
    "#     json.dump(twitter.get_user_info('rngland'), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 20:14:40,021 [\u001b[0;32mINFO\u001b[0m] :: User is authenticated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.logged_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from liked_tweets import liked_tweets\n",
    "liked_tweets = liked_tweets.locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('liked_tweets.py', 'r') as file:\n",
    "    file_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "exec(file_contents, my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to share my non-rigorous vibes based approach towards data science modeling of data. A lot of my job as a staff data scientist (i'm staff btw) is doing a vibe-check on technical problems, and helping other data scientists from going down an intractable path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://github.com/deepfates/memery'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'https://twitter.com/tdinh_me/status/1705155679819084179'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'tdinh_me'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'1705155679819084179'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indie hacker.\n",
      "\n",
      "🧠 @TypingMindApp $30K/mo\n",
      "📸 @XnapperHQ $6K/mo\n",
      "🛠 @devutils_app $5.5K/mo\n",
      "🪄 @blackmagic_so (Sold, was $14K/mo)\n",
      "\n",
      "💬 https://t.co/9b1SSybSdG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fri Sep 22 09:42:48 +0000 2023'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'https://pbs.twimg.com/profile_images/1681138725047136257/5f0aG3LH_normal.jpg'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(my_dict['liked']['data'][8]['content']['itemContent']['tweet_results']['result']['legacy']['full_text'])\n",
    "\n",
    "urls = my_dict['liked']['data'][3]['content']['itemContent']['tweet_results']['result']['legacy']['entities']['urls']\n",
    "\n",
    "for i in urls:\n",
    "    i['expanded_url']\n",
    "\n",
    "# [i for i in urls['expanded_url']]\n",
    "\n",
    "screen_name = my_dict['liked']['data'][0]['content']['itemContent']['tweet_results']['result']['core']['user_results']['result']['legacy']['screen_name']\n",
    "rest_id = my_dict['liked']['data'][0]['content']['itemContent']['tweet_results']['result']['rest_id']\n",
    "\n",
    "'https://twitter.com/' + screen_name + '/status/' + rest_id\n",
    "\n",
    "screen_name\n",
    "rest_id\n",
    "\n",
    "print(my_dict['liked']['data'][0]['content']['itemContent']['tweet_results']['result']['core']['user_results']['result']['legacy']['description'])\n",
    "\n",
    "\n",
    "\n",
    "my_dict['liked']['data'][0]['content']['itemContent']['tweet_results']['result']['legacy']['created_at']\n",
    "\n",
    "#  'in_reply_to_screen_name': 'ryxcommar',\n",
    "# 'in_reply_to_status_id_str': '1700002950012145747',\n",
    "\n",
    "# my_dict['liked']['data'][0]['content']['itemContent']['tweet_results']['result']['core']['user_results']['result']['is_blue_verified']\n",
    "\n",
    "\n",
    "my_dict['liked']['data'][3]['content']['itemContent']['tweet_results']['result']['core']['user_results']['result']['legacy']['profile_image_url_https']\n",
    "\n",
    "\n",
    "# card  \n",
    "    #   {'key': 'title',\n",
    "    #        'value': {'string_value': 'My solopreneur story: zero to $45K/mo in 2 years',\n",
    "    #   {'key': 'description',\n",
    "    #        'value': {'string_value': 'Today is exactly 2 years since I quit my job and become a full-time indie hacker.',\n",
    "        #   {'key': 'photo_image_full_size_alt_text',\n",
    "        #    'value': {'string_value': 'Search over large image datasets with natural language and computer vision! - GitHub - deepfates/memery: Search over large image datasets with natural language and computer vision!',\n",
    "\n",
    "\n",
    "        #      {'key': 'thumbnail_image',\n",
    "        #    'value': {'image_value': {'height': 200,\n",
    "        #      'width': 400,\n",
    "        #      'url': 'https://pbs.twimg.com/card_img/1704513029398745088/aRyYEIOj?format=jpg&name=400x400'},\n",
    "        #     'type': 'IMAGE'}},\n",
    "#card \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32md:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3460\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[5], line 3\u001b[0m\n    liked_tweets = ast.literal_eval(f.read())\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32md:\\Anaconda\\lib\\ast.py:59\u001b[0m in \u001b[0;35mliteral_eval\u001b[0m\n    node_or_string = parse(node_or_string, mode='eval')\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32md:\\Anaconda\\lib\\ast.py:47\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<unknown>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    liked_tweets = {'data': [{'entryId': 'tweet-1705155679819084179',\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "with open(\"liked_tweets.py\", \"r\") as f:\n",
    "    liked_tweets = ast.literal_eval(f.read())\n",
    "liked_tweets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 13 column 35 (char 587)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mliked_tweets.py\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m   liked_tweets \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mload(f)\n\u001b[0;32m      4\u001b[0m liked_tweets\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\json\\__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 13 column 35 (char 587)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"liked_tweets.py\", \"r\") as f:\n",
    "  liked_tweets = json.load(f)\n",
    "liked_tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
